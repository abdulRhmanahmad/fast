from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from openai import OpenAI
import os

app = FastAPI()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))

# ğŸ”¸ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø© Ø§Ù„ØªÙŠ ØªÙÙ‚ÙŠÙ‘Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
system_prompt = (
    "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø­Ø¬Ø² ØªØ§ÙƒØ³ÙŠ Ù…Ø­ØªØ±Ù. "
    "Ù„Ø§ ØªØªØ­Ø¯Ø« Ø¥Ù„Ø§ Ø¹Ù† Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ§ÙƒØ³ÙŠ ÙˆØ§Ù„Ø±Ø­Ù„Ø§Øª. "
    "Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨ Ù…Ù†Ùƒ Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹ Ø®Ø§Ø±Ø¬ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø·Ø§Ù‚ØŒ Ø§Ø¹ØªØ°Ø± Ø¨Ø£Ø¯Ø¨ ÙˆÙƒØ±Ø± Ø£Ù†Ùƒ Ù…Ø®ØªØµ Ø¨Ø§Ù„Ø­Ø¬Ø² ÙÙ‚Ø·."
)

class MessageRequest(BaseModel):
    message: str

class MessageResponse(BaseModel):
    response: str

@app.post("/chat", response_model=MessageResponse)
async def chat(req: MessageRequest):
    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            temperature=0.5,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user",   "content": req.message}
            ]
        )
        reply = completion.choices[0].message.content
        return MessageResponse(response=reply)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
